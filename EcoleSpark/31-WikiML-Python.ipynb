{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Wikipedia pageviews (for mobile and for desktop)\n",
    "\n",
    "Dataset: This file contains a count of pageviews to the English-language Wikipedia over nearly 6 weeks, grouped by timestamp (down to a one-second resolution level) and site (mobile or desktop).\n",
    " * from 2015-03-16T00:00:00 Monday\n",
    " * stop 2015-04-25T15:59:59 Saturday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the DataSet.\n",
    "---\n",
    "We will not cache this dataset.  In a later step we will cache just the aggregate data, saving memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pageViewsDF = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").csv(\"/data/training/pageviews-by-second-tsv.gz\")\n",
    "pageViewsDF = pageViewsDF\\\n",
    "    .withColumn(\"timestamp\", fn.col(\"timestamp\").cast(TimestampType()))\\\n",
    "    .withColumn(\"requests\", pageViewsDF.requests.cast(\"int\"))\n",
    "\n",
    "pageViewsDF.printSchema()\n",
    "pageViewsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageViewsDF.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Let's graph the traffic for the desktop and mobile websites, as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ax = plt.subplots()\n",
    "\n",
    "data = pageViewsDF.filter(\"site == 'mobile'\").select(\"timestamp\", \"requests\").take(200)\n",
    "time = [row[0] for row in data]\n",
    "requests = [row[1] for row in data]\n",
    "ax.scatter(time, requests, marker='o', linestyle='--', color='r', label='Power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reduce our data to hourly totals, and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourCol = fn.hour(fn.col(\"timestamp\")).alias(\"Hour\")\n",
    "dayCol = fn.date_format(fn.col(\"timestamp\"), \"E\").alias(\"Day\")\n",
    "dateCol = fn.to_date(fn.col(\"timestamp\")).alias(\"Date\")\n",
    "# This next line is ugly, but what does is create a timestamp rounded to the nearest hour.\n",
    "# We could create a python UDF, but it's faster if we can reuse the built-in Scala UDFs.\n",
    "dateTimeCol = fn.from_unixtime(dateCol.cast(TimestampType()).cast(LongType()) + hourCol * 60 * 60).alias(\"DateTime\")\n",
    "\n",
    "requestsPerHourDF=pageViewsDF.groupBy(dayCol, dateTimeCol).sum(\"requests\").withColumnRenamed(\"sum(requests)\", \"TotalRequests\").orderBy(\"DateTime\")\n",
    "requestsPerHourDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requestsPerHourDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requestsPerHourDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Graph the requests as a function of time.\n",
    "Notice several key features:\n",
    "* Traffic cycles up and down by Time-of-Day\n",
    "* Traffic cycles up and down by Day-of-Week\n",
    "* And we might imagine traffic increasing month-to-month, as Wikipedia becomes ever more popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# anaylyse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Extract numerical features we can use for machine learning.\n",
    "This involves extracting the hour, day-of-week, and date-time all as Doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourCol = fn.hour(fn.col(\"DateTime\")).cast(DoubleType()).alias(\"Hour\")\n",
    "dayCodeCol = (fn.date_format(fn.col(\"DateTime\"), \"u\")-1).cast(DoubleType()).alias(\"DayCode\")\n",
    "unixTimeCol = fn.col(\"DateTime\").cast(TimestampType()).cast(DoubleType()).alias(\"UnixTime\")\n",
    "totalCol = fn.col(\"TotalRequests\").cast(DoubleType()).alias(\"TotalRequests\")\n",
    "\n",
    "requestsPerHourNumericalDF=requestsPerHourDF.select(hourCol, dayCodeCol, unixTimeCol, \"Day\", \"DateTime\", totalCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "requestsPerHourNumericalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build a linear-regression machine learning pipeline.\n",
    "We expect linear growth over time.  But a cyclical pattern based on hour-of-day and day-of-week.  \n",
    "To account for the non-linear relationship for hour and day-of-week, we'll encode each hour-of-day as an independent variable in a vector.  This is called OneHotEncoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "hourEncoder = OneHotEncoder(inputCol=\"Hour\", outputCol=\"HourVector\")\n",
    "dayEncoder = OneHotEncoder(inputCol=\"DayCode\", outputCol=\"DayVector\")\n",
    "\n",
    "# Selects columns from the input dataframe and puts them into a Vector we can use for learning.\n",
    "vectorizer = VectorAssembler(inputCols=[\"UnixTime\", \"DayVector\", \"HourVector\"], outputCol=\"features\")\n",
    "\n",
    "# Scales the data to fit a standard Guassian curve.  This ensures large inputs don't dwarf small inputs.\n",
    "standardizer = StandardScaler(inputCol='features', outputCol='standardizedFeatures')\n",
    "\n",
    "# The linear regressoin \n",
    "linearReg = LinearRegression(featuresCol = 'standardizedFeatures', labelCol = 'TotalRequests')\n",
    "linearReg.setPredictionCol(\"PredictedTotal\")\n",
    "linearReg.setRegParam(.5)\n",
    "\n",
    "pipeline = Pipeline().setStages([dayEncoder, hourEncoder, vectorizer, standardizer, linearReg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build a model using our machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(requestsPerHourNumericalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict traffic based on time, day-of-week, and hour-of-day, using model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict traffic using our model.  At first we'll predict using the entire dataset.\n",
    "result=model.transform(requestsPerHourNumericalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.select(\"Day\", \"DateTime\", \"TotalRequests\", \"PredictedTotal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.select(\"Day\", \"DateTime\", \"TotalRequests\", \"PredictedTotal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate our model using Training and Test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split up our data into Training Data and Test Data\n",
    "\n",
    "(trainingData, testData) = requestsPerHourNumericalDF.randomSplit((0.80, 0.20), seed = 42)\n",
    "(trainingData.count(), testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a model using the Training Data\n",
    "testModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict traffic using our model.  At first we'll predict using the entire dataset.\n",
    "testResult=testModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testResult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testResult.select(\"Day\", \"DateTime\", \"TotalRequests\", \"PredictedTotal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testResult.select(\"Day\", \"DateTime\", \"TotalRequests\", \"PredictedTotal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's compute some evaluation metrics against our test dataset\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(testResult.rdd.map(lambda r: (r.PredictedTotal, r.TotalRequests)))\n",
    "\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "explainedVariance = metrics.explainedVariance\n",
    "r2 = metrics.r2\n",
    "\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "print(\"Explained Variance: {}\".format(explainedVariance))\n",
    "print(\"R2: {}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's look at the distribution of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of error.\n",
    "# First we calculate the residual error and divide it by the RMSE\n",
    "testResult.selectExpr(\"TotalRequests\", \"PredictedTotal\", \"TotalRequests - PredictedTotal Residual_Error\", \"(TotalRequests - PredictedTotal) / {} -.25   Within_RSME\".format(rmse)).registerTempTable(\"RMSE_Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * from RMSE_Evaluation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Challenge Exercise 1:\n",
    "Instead of using LinearRegression, try using [`org.apache.spark.ml.regression.DecisionTreeRegressor`]()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "WikiML-Python",
  "notebookId": 311802
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
