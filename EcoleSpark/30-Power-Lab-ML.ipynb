{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Power Plant ML Pipeline Application\n",
    "=====\n",
    "This is an end-to-end example of using a number of different machine learning algorithms to solve a supervised regression problem.\n",
    "\n",
    "Table of Contents\n",
    "----\n",
    "- *Step 1: Business Understanding*\n",
    "- *Step 2: Extract-Transform-Load (ETL) Your Data*\n",
    "- *Step 3: Explore Your Data*\n",
    "- *Step 4: Visualize Your Data*\n",
    "- *Step 5: Data Preparation*\n",
    "- *Step 6: Data Modeling*\n",
    "\n",
    "\n",
    "*We are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.  Power generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid.*\n",
    "\n",
    "More information about Peaker or Peaking Power Plants can be found on Wikipedia https://en.wikipedia.org/wiki/Peaking_power_plant\n",
    "\n",
    "\n",
    "Given this business problem, we need to translate it to a Machine Learning task.  The ML task is regression since the label (or target) we are trying to predict is numeric.\n",
    "\n",
    "\n",
    "The example data is provided by UCI at [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)\n",
    "\n",
    "You can read the background on the UCI page, but in summary we have collected a number of readings from sensors at a Gas Fired Power Plant\n",
    "\n",
    "(also called a Peaker Plant) and now we want to use those sensor readings to predict how much power the plant will generate.\n",
    "\n",
    "\n",
    "More information about Machine Learning with Spark can be found in the programming guide in the [SparkML Guide](https://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "\n",
    "\n",
    "*Please note this example only works with Spark version 1.4 or higher*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 1: Business Understanding\n",
    "---\n",
    "The first step in any machine learning task is to understand the business need. \n",
    "\n",
    "As described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.\n",
    "\n",
    "The problem is a regression problem since the label (or target) we are trying to predict is numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 2: Extract-Transform-Load (ETL) Your Data\n",
    "---\n",
    "\n",
    "Now that we understand what we are trying to do, the first step is to load our data into a format we can query and use.  This is known as ETL or \"Extract-Transform-Load\".  We will load our file from Amazon s3.\n",
    "\n",
    "Note: Alternatively we could upload our data using \"Databricks Menu > Tables > Create Table\", assuming we had the raw files on our local computer.\n",
    "\n",
    "Our data is available on HDFS at the following path:  \n",
    "`/data/training/CCPP_sample.csv`\n",
    "\n",
    "**ToDo:** Let's start by printing the first 5 lines of the file.  \n",
    "*Hint*: To read the file into an RDD use `sc.textFile(\"/data/training/CCPP_sample.csv\")`  \n",
    "*Hint*: Then you will need to figure out how to `take` and print the first 5 lines of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rawTextRdd = sc.textFile(\"/data/training/CCPP_sample.csv\")\n",
    "for line in rawTextRdd.take(5):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The file is a .ssv (SemiColon Seperated Values) file of floating point numbers.  \n",
    "\n",
    "Our schema definition from UCI appears below:\n",
    "\n",
    "- AT = Atmospheric Temperature in C\n",
    "- V = Exhaust Vacuum Speed\n",
    "- AP = Atmospheric Pressure\n",
    "- RH = Relative Humidity\n",
    "- PE = Power Output.  This is the value we are trying to predict given the measurements above.\n",
    "\n",
    "\n",
    "**ToDo:** Transform the RDD so that each row is a tuple of float values.  Then print the first 5 rows.  \n",
    "*Hint:* Use filter to exclude lines that start with AT to remove the header.  \n",
    "*Hint:* Use map to transform each line into a PowerPlantRow of data fields.  \n",
    "*Hint:* Use python's str.split break up each line into individual fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "PowerPlantRow=namedtuple(\"PowerPlantRow\", [\"AT\", \"V\", \"AP\", \"RH\", \"PE\"])\n",
    "rawDataRdd=rawTextRdd\\\n",
    "  .map(lambda x: x.replace(\",\", \".\").split(\";\"))\\\n",
    "  .filter(lambda line: line[0] != \"AT\")\\\n",
    "  .map(lambda line: PowerPlantRow(float(line[0]), float(line[1]), float(line[2]), float(line[3]), float(line[4])))\n",
    "rawDataRdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 3: Explore Your Data\n",
    "---\n",
    "Now that your data is loaded, let's explore it, verify it, and do some basic analysis and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Transform your `rawDataRdd` into a Dataframe named `power_plant`.  Then use the `display(power_plant)` function to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "powerPlant=rawDataRdd.toDF()\n",
    "powerPlant.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, let's register our dataframe as an SQL table.  Because this lab may be run many times, we'll take the precaution of removing any existing tables first.\n",
    "\n",
    "**ToDo:** Execute the prepared code in the following cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS power_plant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Register your `powerPlant` dataframe as the table named `power_plant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "powerPlant.createOrReplaceTempView(\"power_plant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Perform the query `SELECT * FROM power_plant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM power_plant\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Use the `desc power_plant` SQL command to describe the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"desc power_plant\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Schema Definition**\n",
    "\n",
    "Our schema definition from UCI appears below:\n",
    "\n",
    "- AT = Atmospheric Temperature in C\n",
    "- V = Exhaust Vacuum Speed\n",
    "- AP = Atmospheric Pressure\n",
    "- RH = Relative Humidity\n",
    "- PE = Power Output\n",
    "\n",
    "PE is our label or target. This is the value we are trying to predict given the measurements.\n",
    "\n",
    "*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Display summary statistics for the the columns.  \n",
    "*Hint:* To access the table from python use `sqlContext.table(\"power_plant\")`  \n",
    "*Hint:* We can use the describe function with no parameters to get some basic stats for each column like count, mean, max, min and standard deviation. The describe function is a method attached to a dataframe. More information can be found in the [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.table(\"power_plant\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 4: Visualize Your Data\n",
    "---\n",
    "\n",
    "To understand our data, we will look for correlations between features and the label.  This can be important when choosing a model.  E.g., if features and a label are linearly correlated, a linear model like Linear Regression can do well; if the relationship is very non-linear, more complex models such as Decision Trees can be better. We use Databrick's built in visualization to view each of our predictors in relation to the label column as a scatter plot to see the correlation between the predictors and the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Do a scatter plot of Power(PE) as a function of Temperature (AT).  \n",
    "*Bonus:* Name the y-axis \"Power\" and the x-axis \"Temperature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ax = plt.subplots()\n",
    "\n",
    "data = spark.sql(\"select AT as Temperature, PE as Power from power_plant\").collect()\n",
    "temperature = [list(r)[0] for r in data]\n",
    "power = [list(r)[1] for r in data]\n",
    "ax.scatter(temperature, power, marker='o', linestyle='--', color='r', label='Power')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice there appears to be a strong linear correlation between temperature and Power Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Do a scatter plot of Power(PE) as a function of ExhaustVacuum (V).  \n",
    "*Bonus:* Name the y-axis \"Power\" and the x-axis \"ExhaustVacuum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"select V as ExhaustVacuum, PE as Power from power_plant;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some semblance of a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Do a scatter plot of Power(PE) as a function of Pressure (AP).  \n",
    "*Bonus:* Name the y-axis \"Power\" and the x-axis \"Pressure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"select AP as Pressure, PE as Power from power_plant;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ToDo:** Do a scatter plot of Power(PE) as a function of Humidity (RH).  \n",
    "*Bonus:* Name the y-axis \"Power\" and the x-axis \"Humidity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"select RH Humidity, PE Power from power_plant;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "...and atmospheric pressure and relative humidity seem to have little to no linear correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Step 5: Data Preparation\n",
    "---\n",
    "\n",
    "The next step is to prepare the data. Since all of this data is numeric and consistent this is a simple task for us today.\n",
    "\n",
    "We will need to convert the predictor features from columns to Feature Vectors using [pyspark.ml.feature.VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler)\n",
    "\n",
    "The VectorAssembler will be the first step in building our ML pipeline.\n",
    "\n",
    "**ToDo:** Assign the data frame to the local variable `dataset`  \n",
    "**ToDo:** Create a VectorAssembler named `vectorizer` to read your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "#Finish here...\n",
    "dataset=None\n",
    "vectorizer=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##Step 6: Data Modeling\n",
    "Now let's model our data to predict what the power output will be given a set of sensor readings\n",
    "\n",
    "Our first model will be based on simple linear regression since we saw some linear patterns in our data based on the scatter plots during the exploration stage.\n",
    "\n",
    "In machine learning we often will split up our initial data set into a \"trainingSet\" used to train our model and a \"testSet\" to evaluate the model's performance in giving predictions.\n",
    "\n",
    "**ToDo:** Divide up the model into a trainingSet and a testSet.  Then cache each set into memory to maximize performance.   \n",
    "**Hint:** Use the [randomSplit](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) function.\n",
    "**Hint:** The data set won't actually be cached until you perform an action on the dataset.  \n",
    "*Tip*: For reproducability it's often best to use a predefined seed when doing the random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(trainingSet, testSet) = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we'll create a Linear Regression Model and use the built in help to identify how to train it.  See more details at [Linear Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) in the ML guide.\n",
    "\n",
    "**ToDo**: Run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ***** LINEAR REGRESSION MODEL ****\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Let's initialize our linear regression learner\n",
    "lr = LinearRegression()\n",
    "\n",
    "# We use explain params to dump the parameters we can use\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The cell below is based on the Spark ML pipeline API. More information can be found in the Spark ML Programming Guide at https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "**ToDo:** Read, understand, and then run the next cell to train our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now we set the parameters for the method\n",
    "lr.setPredictionCol(\"Predicted_PE\")\\\n",
    "  .setLabelCol(\"PE\")\\\n",
    "  .setMaxIter(100)\\\n",
    "  .setRegParam(0.1)\n",
    "\n",
    "# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\n",
    "lrPipeline = Pipeline()\n",
    "lrPipeline.setStages([vectorizer, lr])\n",
    "\n",
    "# Let's first train on the entire dataset to see what we get\n",
    "lrModel = lrPipeline.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's apply this model to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictionsAndLabels = lrModel.transform(testSet)\n",
    "\n",
    "predictionsAndLabels.select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since Linear Regression is Simply a Line of best fit over the data that minimizes the square of the error, given multiple input dimensions we can express each predictor as a line function of the form:\n",
    "\n",
    "\\\\[ y = a + b x_1 + b x_2 + b x_i ... \\\\]\n",
    "\n",
    "where a is the intercept and b are coefficients.\n",
    "\n",
    "To express the coefficients of that line we can retrieve the Estimator stage from the PipelineModel and express the weights and the intercept for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The intercept is as follows:\n",
    "intercept = lrModel.stages[1].intercept\n",
    "print(intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The coefficents (i.e. weights) are as follows:\n",
    "\n",
    "weights = lrModel.stages[1].weights.toArray()\n",
    "\n",
    "\n",
    "featuresNoLabel = [col for col in dataset.columns if col != \"PE\"]\n",
    "\n",
    "coefficents = sc.parallelize(weights).zip(sc.parallelize(featuresNoLabel))\n",
    "\n",
    "# Now let's sort the coeffecients from the most to the least\n",
    "\n",
    "equation = \"y = {intercept}\".format(intercept=intercept)\n",
    "variables = []\n",
    "for x in coefficents.sortByKey().collect():\n",
    "    weight = abs(x[0])\n",
    "    name = x[1]\n",
    "    symbol = \"+\" if (x[0] > 0) else \"-\"\n",
    "    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n",
    "\n",
    "# Finally here is our equation\n",
    "print(\"Linear Regression Equation: \" + equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on examining the output it shows there is a strong negative correlation between Atmospheric Temperature (AT) and Power Output.\n",
    "\n",
    "But our other dimenensions seem to have little to no correlation with Power Output. Do you remember **Step 2: Explore Your Data**? When we visualized each predictor against Power Output using a Scatter Plot, only the temperature variable seemed to have a linear correlation with Power Output so our final equation seems logical.\n",
    "\n",
    "\n",
    "Now let's see what our predictions look like given this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictionsAndLabels = lrModel.transform(testSet)\n",
    "\n",
    "predictionsAndLabels.select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have real predictions we can use an evaluation metric such as Root Mean Squared Error to validate our regression model. The lower the Root Mean Squared Error, the better our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now let's compute some evaluation metrics against our test dataset\n",
    "\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "metrics = RegressionMetrics(predictionsAndLabels.select(\"Predicted_PE\", \"PE\").rdd.map(lambda r: (float(r[0]), float(r[1]))))\n",
    "\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "explainedVariance = metrics.explainedVariance\n",
    "r2 = metrics.r2\n",
    "\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "print(\"Explained Variance: {}\".format(explainedVariance))\n",
    "print(\"R2: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generally a good model will have 68% of predictions within 1 RMSE and 95% within 2 RMSE of the actual value. Let's calculate and see if a RMSE of 4.51 meets this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First we calculate the residual error and divide it by the RMSE\n",
    "predictionsAndLabels.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", \"(PE - Predicted_PE) / {} Within_RSME\".format(rmse)).registerTempTable(\"Power_Plant_RMSE_Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * from Power_Plant_RMSE_Evaluation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"-- Now we can display the RMSE as a Histogram. Clearly this shows that the RMSE is centered around 0 with the vast majority of the error within 2 RMSEs.\n",
    "SELECT Within_RSME  from Power_Plant_RMSE_Evaluation\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see this definitively if we count the number of predictions within + or - 1.0 and + or - 2.0 and display this as a pie chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT case when Within_RSME <= 1.0 and Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 and Within_RSME >= -2.0 then 2 else 3 end RSME_Multiple, COUNT(*) count  from Power_Plant_RMSE_Evaluation\n",
    "group by case when Within_RSME <= 1.0 and Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 and Within_RSME >= -2.0 then 2 else 3 end\"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So we have 68% of our training data within 1 RMSE and 97% (68% + 29%) within 2 RMSE. So the model is pretty decent. Let's see if we can tune the model to improve it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "Power-Lab3-ML",
  "notebookId": 68203
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
