{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Broadcast Variables?\n",
    "Broadcast Variables allow us to broadcast a read-only copy of non-rdd data to all the executors.  The executors can then access the value of this data locally.  This is much more efficent than relying on the driver to trasmit this data teach time a task is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Broadcast Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a broadcast variable, transmitting it's value to all the executors.\n",
    "broadcastVar=sc.broadcast([1,2,3])\n",
    "\n",
    "# I can read it's value\n",
    "print(broadcastVar.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The value is available on the driver\n",
    "print(\"Driver:\", broadcastVar.value)\n",
    "\n",
    "# And on the executors\n",
    "mapFunction=lambda n: \"Task \" + str(n) + \": \" + str(broadcastVar.value)\n",
    "results=sc.parallelize(range(10), numSlices=10).map(mapFunction).collect()\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Broadcast Variables can improve performance (demo)\n",
    "Here we have a medium sized dataSet, small enough to fit in RAM, but still involves quite a bit of network communication when sending the dataSet to the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a medium sized dataSet of several million values.\n",
    "size=2*1000*1000\n",
    "dataSet=list(range(size))\n",
    "\n",
    "# Check out the size of the dataSet in RAM.\n",
    "import sys\n",
    "print(sys.getsizeof(dataSet) / 1000 / 1000, \"Megabytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate the overhead of network communication when not using broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ceate an RDD with 5 partitions so that we can do an operation in 5 seperate tasks running in parallel on up to 5 different executors.\n",
    "rdd=sc.parallelize([1,2,3,4,5], numSlices=5)\n",
    "print(rdd.getNumPartitions(), \"partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In a loop, do a job 5 times without using broadcast variables...\n",
    "for i in range(5):\n",
    "  rdd.map(lambda x: len(dataSet) * x).collect()\n",
    "\n",
    "# Look how slow it is...\n",
    "# This is because our local \"data\" variable is being used by the lambda and thus must be sent to each executor every time a task is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do that again, but this time we'll first send a copy of the dataset to the executors once, so that the data is available locally every time a task is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a broadcast variable.  This will transmit the dataset to the executors.\n",
    "broadcastVar=sc.broadcast(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the job 5 times, and notice how much faster it is since we don't have to retransmit the data set each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  rdd.map(lambda x: len(broadcastVar.value)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's delete the the broadcast variable out of the Executor JVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Asked Questions about Broadcast Variables\n",
    "**Q:** How is this different than using an RDD to keep data on an executor?  \n",
    "**A:** With an RDD, the data is divided up into partitions and executors hold only a few partitions.  A broadcast variable is sent out to all the executors.\n",
    "\n",
    "**Q:** When should I use an RDD and when should I use a broadcast variable?  \n",
    "**A:** BroadCast variables must fit into RAM (and they're generally under 20 MB).  And they are on all executors.  They're good for small datasets that you can afford to leave in memory on the executors.  RDDs are better for very large datasets that you want to partition and divide up between executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Broadcasts Work with Dataframes?\n",
    "\n",
    "Broadcasts can be used to improve performance of some kinds of joins when using Dataframes/Dataset/SparkSQL.\n",
    "\n",
    "In many we may want to join one or more (relatively) small tables against a single large dataset -- e.g., \"enriching\" a transaction or event table (containing, say, customer IDs and store IDs) with additional \"business fact tables\" (like customer demographic info by ID, and store location and profile by ID). Instead of joining all of these as distributed datasets, typically requiring a shuffle each time, we could broadcast a copy of the small tables to each executor, where they can can be joined directly (through a hash lookup) against the local partitions of the bigger table.\n",
    "\n",
    "This approach is sometimes called a \"map-side join\" or \"hash join\" and is related to, but not the same as, \"skewed join\" in other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Broadcast Joins with Spark\n",
    "\n",
    "By default, Spark will use a shuffle to join two datasets (unless Spark can verify that they are already co-partitioned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = sqlContext.range(100)\n",
    "df2 = sqlContext.range(100)\n",
    "\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Spark UI for that job, and note the stage count and the shuffle.\n",
    "\n",
    "To use a broadcast join, we need at least one of the following:\n",
    "* statistics from running Hive ANALYZE on the table, and the size less than `spark.sql.autoBroadcastJoinThreshold`\n",
    "* statistics from caching the table in Spark, and the size less than `spark.sql.autoBroadcastJoinThreshold`\n",
    "* a broadcast hint applied to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df1.join(broadcast(df2), df1[\"id\"] == df2[\"id\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.cache().count()\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.unpersist()\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "61-Broadcast-demo",
  "notebookId": 616699
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
