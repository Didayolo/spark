{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External modeling libraries\n",
    "---\n",
    "How can we leverage our existing experience with modeling libraries like [scikit-learn](http://scikit-learn.org/stable/index.html)?  We'll explore three approaches that make use of existing libraries, but still benefit from the parallelism provided by Spark.\n",
    "\n",
    "These approaches are:\n",
    " * Grid Search\n",
    " * Cross Validation\n",
    " * Sampling\n",
    " \n",
    "We'll start by using scikit-learn on the driver and then we'll demonstrate the parallel techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Use scikit-learn locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from `sklearn.datasets`, and create test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Generate test and train sets\n",
    "size = len(iris.target)\n",
    "indices = np.random.permutation(size)\n",
    "\n",
    "cutoff = int(size * .30)\n",
    "\n",
    "testX = iris.data[indices[0:cutoff],:]\n",
    "trainX = iris.data[indices[cutoff:],:]\n",
    "testY = iris.target[indices[0:cutoff]]\n",
    "trainY = iris.target[indices[cutoff:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.target[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a nearest neighbors classifier using [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNeighborsClassifier using the default settings\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(trainX, trainY)\n",
    "\n",
    "predictions = knn.predict(testX)\n",
    "\n",
    "# Print out the accuracy of the classifier on the test set\n",
    "print(sum(predictions == testY) / float(len(testY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function `runNearestNeighbors` that takes in a parameter `k` and returns a tuple of (`k`, accuracy).  Note that we'll load the data from `sklearn.datasets`, and we'll create train and test splits using [sklearn.cross_validation.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def runNearestNeighbors(k):\n",
    "    # Load dataset from sklearn.datasets\n",
    "    irisData = datasets.load_iris()\n",
    "    \n",
    "    # Split into train and test using sklearn.cross_validation.train_test_split\n",
    "    yTrain, yTest, XTrain, XTest = train_test_split(irisData.target, \n",
    "                                                    irisData.data)\n",
    "    \n",
    "    # Build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(XTrain, yTrain)\n",
    "    \n",
    "    # Calculate predictions and accuracy\n",
    "    predictions = knn.predict(XTest)\n",
    "    accuracy = (predictions == yTest).sum() / float(len(yTest))\n",
    "    \n",
    "    return (k, accuracy)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a grid search for `k` from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = sc.parallelize(range(1, 11))\n",
    "results = k.map(runNearestNeighbors)\n",
    "print('\\n'.join(map(str, results.collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transfer the data using a Broadcast instead of loading it at each executor.  You can create a [Broadcast](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.Broadcast) variable using [sc.broadcast()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.broadcast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the Broadcast variable\n",
    "irisBroadcast = sc.broadcast(iris)\n",
    "\n",
    "def runNearestNeighborsBroadcast(k):\n",
    "    # Using the data in the irisBroadcast variable split into train and test using\n",
    "    # sklearn.cross_validation.train_test_split\n",
    "    yTrain, yTest, XTrain, XTest = train_test_split(irisBroadcast.value.target,\n",
    "                                                    irisBroadcast.value.data)\n",
    "    \n",
    "    # Build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(XTrain, yTrain)\n",
    "    \n",
    "    # Calculate predictions and accuracy\n",
    "    predictions = knn.predict(XTest)\n",
    "    accuracy = (predictions == yTest).sum() / float(len(yTest))\n",
    "    \n",
    "    return (k, accuracy)   \n",
    "  \n",
    "# Rerun grid search\n",
    "k = sc.parallelize(range(1, 11))\n",
    "results = k.map(runNearestNeighborsBroadcast)\n",
    "print('\\n'.join(map(str, results.collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use [sklearn.cross_validation.KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) to evaluate our model using 10-fold cross validation.  First, generate the 10 folds using `KFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Create indicies for 10-fold cross validation\n",
    "kf = KFold(size, n_folds=10)\n",
    "\n",
    "print(len(kf))\n",
    "\n",
    "print(kf.__iter__())\n",
    "\n",
    "train, test = kf.__iter__().__next__()\n",
    "\n",
    "print(\"First fold:\")\n",
    "\n",
    "print(\"Train:\" + str(train))\n",
    "\n",
    "print(\"Test:\" + str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = sc.parallelize(kf)\n",
    "print(folds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that runs nearest neighbors based on the fold information passed in.  Note that we'll have the function return an [np.array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) which provides us with additional functionality that we'll take advantage of in a couple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def runNearestNeighborsWithFolds(tup):\n",
    "    trainIndex,testIndex = tup\n",
    "    # Assign training and test sets from irisBroadcast using trainIndex and testIndex\n",
    "    XTrain = irisBroadcast.value.data[trainIndex]\n",
    "    yTrain = irisBroadcast.value.target[trainIndex]\n",
    "    XTest = irisBroadcast.value.data[testIndex]\n",
    "    yTest = irisBroadcast.value.target[testIndex]\n",
    "    \n",
    "    # Build the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(XTrain, yTrain)\n",
    "    \n",
    "    # Calculate predictions\n",
    "    predictions = knn.predict(XTest)\n",
    "    \n",
    "    # Compute the number of correct predictions and total predictions\n",
    "    correct = (predictions == yTest).sum() \n",
    "    total = len(testIndex)\n",
    "    \n",
    "    # Return an np.array of the number of correct predictions and total predictions\n",
    "    return np.array([correct, total])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute nearest neighbors using each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run nearest neighbors on each of the folds\n",
    "foldResults = folds.map(runNearestNeighborsWithFolds)\n",
    "print('correct / total\\n' + '\\n'.join(map(str, foldResults.collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate the results from the folds to see overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that using .sum() on an RDD of numpy arrays sums by columns \n",
    "correct, total = foldResults.sum()\n",
    "print(correct / float(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have a dataset that is too large where we can't use our external modeling library on the full data set.  In this case we might want to build several models on samples of the dataset.  We could either build the same model, using different parameters, or try completely different techniques to see what works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll parallelize the iris dataset and distribute it across our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the iris dataset into 8 partitions\n",
    "irisData = sc.parallelize(zip(iris.target, iris.data), 8)\n",
    "print(irisData.take(2), '\\n')\n",
    "\n",
    "# View the number of elements found in each of the eight partitions\n",
    "print (irisData\n",
    "       .mapPartitions(lambda x: [len(list(x))])\n",
    "       .collect())\n",
    "\n",
    "# View the target (y) stored by partition\n",
    "print('\\n', irisData.keys().glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each of the partitions represents a dataset that we'll be using to run our local model, we have a problem.  The data is ordered, so our partitions are mostly homogenous with regard to our target variable.\n",
    "\n",
    "We'll repartition the data using `partitionBy` so that the data is randomly ordered across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly reorder the data across partitions\n",
    "randomOrderData = (irisData\n",
    "                   .map(lambda x: (np.random.randint(5), x))\n",
    "                   .partitionBy(5)\n",
    "                   .values())\n",
    "\n",
    "# Show the new groupings of target variables\n",
    "print(randomOrderData.keys().glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll build a function that takes in the target and data from the `randomOrderData` RDD and returns the number of correct and total predictions (with regard to a test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall what randomOrderData contains\n",
    "print(randomOrderData.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runNearestNeighborsPartition(labelAndFeatures):\n",
    "    y, X = zip(*labelAndFeatures)\n",
    "    yTrain, yTest, XTrain, XTest = train_test_split(y, X)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(XTrain, yTrain)\n",
    "    \n",
    "    predictions = knn.predict(XTest)\n",
    "    correct = (predictions == yTest).sum() \n",
    "    total = len(yTest)\n",
    "    return [np.array([correct, total])]\n",
    "\n",
    "sampleResults = randomOrderData.mapPartitions(runNearestNeighborsPartition)\n",
    "\n",
    "print('correct / total\\n' + '\\n'.join(map(str, sampleResults.collect())))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "80-scikit-learn",
  "notebookId": 823832
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
